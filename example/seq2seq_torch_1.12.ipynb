{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11310,"status":"ok","timestamp":1666231582585,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"ksS3EtrxfnIe"},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import spacy\n","import random\n","from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n","from torchtext.data.metrics import bleu_score\n","import sys\n","import os\n","import re\n","import torchtext\n","\n","from torchtext.data.utils import get_tokenizer\n","from collections import Counter\n","from torchtext.vocab import vocab\n","import io\n","import pandas as pd\n","\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1666231582586,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"CnMXiA-PAKtO","outputId":"e13779d3-0330-43b2-e181-6387a07e330f"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.1+cu113\n","0.13.1\n"]}],"source":["import torch, torchtext\n","print(torch.__version__)\n","print(torchtext.__version__)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25933,"status":"ok","timestamp":1666231608507,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"l1od3qab8gDm","outputId":"2f524f09-b167-4418-fbb0-99d8532b4632"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25658,"status":"ok","timestamp":1666231644702,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"qxYpSjN6896L","outputId":"178da437-3ba1-471f-a6c1-6c59a0f8a557"},"outputs":[{"output_type":"stream","name":"stdout","text":["2022-10-20 02:07:00.978534: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting zh-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.4.0/zh_core_web_sm-3.4.0-py3-none-any.whl (48.4 MB)\n","\u001b[K     |████████████████████████████████| 48.4 MB 2.2 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-sm==3.4.0) (3.4.1)\n","Collecting spacy-pkuseg<0.1.0,>=0.0.27\n","  Downloading spacy_pkuseg-0.0.32-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (8.1.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.0.9)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.7)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.1)\n","Installing collected packages: spacy-pkuseg, zh-core-web-sm\n","Successfully installed spacy-pkuseg-0.0.32 zh-core-web-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('zh_core_web_sm')\n","2022-10-20 02:07:17.979014: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.4)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!python -m spacy download zh_core_web_sm\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3455,"status":"ok","timestamp":1666231648142,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"WBVfgDsQDhV6"},"outputs":[],"source":["\n","def getJsonFile(filePath,dir_path = \"/content/drive/MyDrive/Colab Notebooks/ithome/torchtext_anki/\"):\n","  return pd.read_json(dir_path+filePath, lines=True, orient='records')\n","\n","train_json = getJsonFile(\"anki_train.json\")\n","test_json = getJsonFile(\"anki_test.json\")\n","\n","\n","total_json = pd.concat([train_json, test_json], axis=0)\n","\n","zh_tokenizer = get_tokenizer('spacy', language='zh_core_web_sm')\n","en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n","\n","def tokenize_eng(text):\n","    text = re.sub(r\"([.!?])\", r\" \\1\", text.lower())\n","    return en_tokenizer(text)\n","\n","def tokenize_zh(text):\n","    # print(text)\n","    regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n","    text = regex.sub(' ', text.lower())\n","    return zh_tokenizer(text)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18881,"status":"ok","timestamp":1666231667018,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"-mJclXQq4GjB","outputId":"25165113-db06-46af-e2b5-05cbba424b41"},"outputs":[{"output_type":"stream","name":"stdout","text":["中文語料的字元表長度:  14461 , 英文的字元表長度:  6933\n"]}],"source":["\n","def build_vocab(sentence_list, tokenizer):\n","  counter = Counter()\n","  for string_ in sentence_list:\n","    counter.update(tokenizer(string_))\n","  return vocab(counter, min_freq =1 , specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n","\n","# chinese.build_vocab(train_data, max_size=50000, min_freq=50, vectors=\"glove.6B.100d\")\n","en_vocab = build_vocab(total_json.English, tokenize_eng)\n","zh_vocab = build_vocab(total_json.Chinese, tokenize_zh)\n","\n","\n","print (\"中文語料的字元表長度: \" , len(zh_vocab.vocab) , \", 英文的字元表長度: \" ,len(en_vocab.vocab))\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15278,"status":"ok","timestamp":1666231682291,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"VtSjD7ONLBEI","outputId":"4cc51c48-0658-44ca-9c27-0d172a9404e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample English: ['我', '最近', '忙', '得', '很'] => Chinese: ['i', \"'ve\", 'been', 'very', 'busy', 'lately', '.']\n"]}],"source":["\n","def data_process(sentence_list):\n","  data = []\n","  for _ , s in sentence_list.iterrows():\n","    data.append((tokenize_zh(s.Chinese),  tokenize_eng(s.English)))\n","    # data.append({\"zh\" : tokenize_zh(s.Chinese), \"en\" : tokenize_eng(s.English) })\n","  return data\n","\n","train_data = data_process(train_json)\n","test_data = data_process(test_json)\n","\n","print (\"Sample English:\", test_data[0][0] , \"=> Chinese:\", test_data[0][1])\n","\n","# val_data = data_process(val_filepaths)\n","# test_data = data_process(test_filepaths)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1666231682291,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"0CBvoBGvPu1p","outputId":"142ffe10-4572-43fe-e85a-26227e9c6061"},"outputs":[{"output_type":"stream","name":"stdout","text":["<bos>\n"]},{"output_type":"execute_result","data":{"text/plain":["4291"]},"metadata":{},"execution_count":8}],"source":["print(en_vocab.vocab.get_itos()[1])\n","en_vocab.vocab.get_stoi()['arabic']"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1666231682292,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"5fy7JnW0fpLt"},"outputs":[],"source":["def translate_sentence(model, sentence_token, zh_vocab, en_vocab, device, max_length=25):\n","\n","    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n","    # if type(sentence) == str:\n","    #     tokens = tokenize_zh(sentence)\n","    # else:\n","    #     tokens = [token.lower() for token in sentence]\n","\n","    # 加入開始符號跟結束符號，代表一個句子\n","    sentence_token.insert(0, '<bos>')\n","    sentence_token.append('<eos>')\n","\n","    # 然後把文字轉自 vactor\n","    text_to_indices = [zh_vocab.vocab.get_stoi()[token] for token in sentence_token]\n","\n","    # 再把 vactor list 轉換成 Tensor\n","    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n","\n","    # 取消梯度修正\n","    with torch.no_grad():\n","        hidden, cell = model.encoder(sentence_tensor)\n","\n","    # 先宣告 outputs ，然後裡面放一個開符號\n","    outputs = [en_vocab.vocab.get_stoi()[\"<bos>\"]]\n","\n","\n","    for _ in range(max_length):\n","        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n","\n","        # seq2seq 的decoder 會把上一個 hidden_state 跟 cell 當作input 這是觀念\n","        # 然後output機率最大的\n","        with torch.no_grad():\n","            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n","            best_guess = output.argmax(1).item()\n","\n","        # 機率最大的數值再把它放進output\n","        outputs.append(best_guess)\n","\n","        # 如果是結束字元 eos 的話就中斷不然會一直預測下去\n","        if output.argmax(1).item() == en_vocab.vocab.get_stoi()[\"<eos>\"]:\n","            break\n","\n","    # 再把 vactor 轉成文字， itos = integer to string\n","    translated_sentence = [en_vocab.vocab.get_itos()[idx] for idx in outputs]\n","\n","    # remove start token\n","    return translated_sentence[1:]\n","\n","\n","\n","def bleu(data, model, tokenize_zh, zh_vocab, en_vocab, device):\n","    targets = []\n","    outputs = []\n","\n","    for example in data:\n","        src = vars(example)[\"zh\"]\n","        trg = vars(example)[\"eng\"]\n","\n","        src_token = tokenize_zh(src)\n","\n","        prediction = translate_sentence(model, src_token , zh_vocab, en_vocab, device)\n","        prediction = prediction[:-1]  # remove <eos> token\n","\n","        targets.append([trg])\n","        outputs.append(prediction)\n","\n","    return bleu_score(outputs, targets)\n","\n","\n","def save_checkpoint(state, filename=\"/content/drive/MyDrive/Colab Notebooks/ithome/checkpoints/seq2seq_cp.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1666231682292,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"IUR3a6iX9zLD"},"outputs":[],"source":["\n","class Encoder(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n","        super(Encoder, self).__init__()\n","        self.dropout = nn.Dropout(p)\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n","\n","    def forward(self, x):\n","        # x shape: (seq_length, N) where N is batch size\n","\n","        embedding = self.dropout(self.embedding(x))\n","        # embedding shape: (seq_length, N, embedding_size)\n","\n","        outputs, (hidden, cell) = self.rnn(embedding)\n","        # outputs shape: (seq_length, N, hidden_size)\n","\n","        return hidden, cell\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1666231682292,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"LFwTb4UP901T"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(\n","        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n","    ):\n","        super(Decoder, self).__init__()\n","        self.dropout = nn.Dropout(p)\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, hidden, cell):\n","        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n","        # is 1 here because we are sending in a single word and not a sentence\n","        x = x.unsqueeze(0)\n","\n","        embedding = self.dropout(self.embedding(x))\n","        # embedding shape: (1, N, embedding_size)\n","\n","        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n","        # outputs shape: (1, N, hidden_size)\n","\n","        predictions = self.fc(outputs)\n","\n","        # predictions shape: (1, N, length_target_vocabulary) to send it to\n","        # loss function we want it to be (N, length_target_vocabulary) so we're\n","        # just gonna remove the first dim\n","        predictions = predictions.squeeze(0)\n","\n","        return predictions, hidden, cell\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1666231682293,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"4VEYNddf93ww"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, source, target, teacher_force_ratio=0.5):\n","        batch_size = source.shape[1]\n","        target_len = target.shape[0]\n","        target_vocab_size = len(en_vocab.vocab)\n","\n","        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","\n","        hidden, cell = self.encoder(source)\n","\n","        # Grab the first input to the Decoder which will be <SOS> token\n","        x = target[0]\n","\n","        for t in range(1, target_len):\n","            # Use previous hidden, cell as context from encoder at start\n","            output, hidden, cell = self.decoder(x, hidden, cell)\n","\n","            # Store next output prediction\n","            outputs[t] = output\n","\n","            # Get the best word the Decoder predicted (index in the vocabulary)\n","            best_guess = output.argmax(1)\n","\n","            # With probability of teacher_force_ratio we take the actual next word\n","            # otherwise we take the word that the Decoder predicted it to be.\n","            # Teacher Forcing is used so that the model gets used to seeing\n","            # similar inputs at training and testing time, if teacher forcing is 1\n","            # then inputs at test time might be completely different than what the\n","            # network is used to. This was a long comment.\n","            x = target[t] if random.random() < teacher_force_ratio else best_guess\n","\n","        return outputs"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":680,"status":"ok","timestamp":1666231682954,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"U2C4ax1l-DZ1"},"outputs":[],"source":["checkPointPath = \"/content/drive/MyDrive/Colab Notebooks/ithome/checkpoints/seq2seq_cp.pth.tar\"\n","\n","\n","num_epochs = 100\n","learning_rate = 0.001\n","batch_size = 64\n","\n","# Model hyperparameters\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","input_size_encoder = len(zh_vocab.vocab)\n","input_size_decoder = len(en_vocab.vocab)\n","output_size = len(en_vocab.vocab)\n","encoder_embedding_size = 300\n","decoder_embedding_size = 300\n","hidden_size = 1024 # Needs to be the same for both RNN's\n","num_layers = 2\n","enc_dropout = 0.5\n","dec_dropout = 0.0\n","\n","# Tensorboard to get nice loss plot\n","writer = SummaryWriter(f\"/content/drive/MyDrive/Colab Notebooks/ithome/runs/loss_plot\")\n","step = 0\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1666231682955,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"b8fmFdm--FpU"},"outputs":[],"source":["\n","PAD_IDX = zh_vocab.vocab.get_stoi()['<pad>']\n","BOS_IDX = zh_vocab.vocab.get_stoi()['<bos>']\n","EOS_IDX = zh_vocab.vocab.get_stoi()['<eos>']\n","\n","\n","def generate_batch(data_batch):\n","  zh_batch, en_batch = [], []\n","  for (zh_token, en_token) in data_batch:\n","    zh_item = torch.tensor([zh_vocab.vocab.get_stoi()[token] for token in zh_token], dtype=torch.long)\n","    zh_batch.append(torch.cat([torch.tensor([BOS_IDX]), zh_item, torch.tensor([EOS_IDX])], dim=0))\n","\n","    en_item = torch.tensor([en_vocab.vocab.get_stoi()[token] for token in en_token], dtype=torch.long)\n","    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n","\n","  zh_batch = pad_sequence(zh_batch, padding_value=PAD_IDX)\n","  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n","  return zh_batch, en_batch\n","\n","train_iter = DataLoader(train_data, batch_size=batch_size,\n","                        shuffle=True, collate_fn=generate_batch)\n","\n","\n","test_iter = DataLoader(test_data, batch_size=batch_size,\n","                        shuffle=True, collate_fn=generate_batch)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":530,"status":"ok","timestamp":1666231683477,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"AKCsMQV8-QII"},"outputs":[],"source":["\n","encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\n","\n","decoder_net = Decoder(\n","    input_size_decoder,\n","    decoder_embedding_size,\n","    hidden_size,\n","    output_size,\n","    num_layers,\n","    dec_dropout,\n",").to(device)\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1046,"status":"error","timestamp":1666231796213,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"C5neDfUY-cfQ","colab":{"base_uri":"https://localhost:8080/","height":500},"outputId":"5def9bd5-a355-45ce-8299-1261073fb247"},"outputs":[{"output_type":"stream","name":"stdout","text":["=> Loading checkpoint\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-d7ce739c41c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckPointPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckPointPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-fcd0184520c4>\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(checkpoint, model, optimizer)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=> Loading checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1605\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Seq2Seq:\n\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([13551, 300]) from checkpoint, the shape in current model is torch.Size([14461, 300]).\n\tsize mismatch for decoder.embedding.weight: copying a param with shape torch.Size([6609, 300]) from checkpoint, the shape in current model is torch.Size([6933, 300]).\n\tsize mismatch for decoder.fc.weight: copying a param with shape torch.Size([6609, 1024]) from checkpoint, the shape in current model is torch.Size([6933, 1024]).\n\tsize mismatch for decoder.fc.bias: copying a param with shape torch.Size([6609]) from checkpoint, the shape in current model is torch.Size([6933])."]}],"source":["\n","model = Seq2Seq(encoder_net, decoder_net).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","pad_idx = en_vocab.vocab.get_stoi()[\"<pad>\"]\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","\n","if os.path.isfile(checkPointPath):\n","    load_checkpoint(torch.load(checkPointPath), model, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyaFkzqH9JvE","executionInfo":{"status":"aborted","timestamp":1666231687986,"user_tz":-480,"elapsed":10,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"}}},"outputs":[],"source":["\n","sentence = \"你想不想要来我家看猫?或者一起看电影?\"\n","example_token = tokenize_zh(sentence)\n","\n","for epoch in range(num_epochs):\n","    print(f\"[Epoch {epoch} / {num_epochs}]\")\n","\n","    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n","    save_checkpoint(checkpoint)\n","\n","    model.eval()\n","\n","    translated_sentence = translate_sentence(\n","        model, example_token, zh_vocab, en_vocab, device, max_length=50\n","    )\n","\n","    print(f\"Translated example sentence: \\n {translated_sentence}\")\n","\n","    model.train()\n","\n","    for batch_idx, batch in enumerate(train_iter):\n","        # Get input and targets and get to cuda\n","        inp_data = batch[0].to(device)\n","        target = batch[1].to(device)\n","\n","        # Forward prop\n","        output = model(inp_data, target)\n","        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n","        # doesn't take input in that form. For example if we have MNIST we want to have\n","        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n","        # way that we have output_words * batch_size that we want to send in into\n","        # our cost function, so we need to do some reshapin. While we're at it\n","        # Let's also remove the start token while we're at it\n","        output = output[1:].reshape(-1, output.shape[2])\n","        target = target[1:].reshape(-1)\n","\n","        optimizer.zero_grad()\n","        loss = criterion(output, target)\n","\n","        # Back prop\n","        loss.backward()\n","\n","        # Clip to avoid exploding gradient issues, makes sure grads are\n","        # within a healthy range\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","        # Gradient descent step\n","        optimizer.step()\n","\n","        # Plot to tensorboard\n","        writer.add_scalar(\"Training loss\", loss, global_step=step)\n","        step += 1\n","\n","\n","score = bleu(test_data[1:100], model, chinese, english, device)\n","print(f\"Bleu score {score*100:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYKI3PhnAWay"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1zSu7vi5lcdb0xGGNFK0TyrJGo4LVEQsq","timestamp":1666140727442}],"mount_file_id":"1zSu7vi5lcdb0xGGNFK0TyrJGo4LVEQsq","authorship_tag":"ABX9TyMRvR0a1j/R1C/oI2NLWG/V"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}