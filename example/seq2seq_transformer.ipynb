{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1a-boZzC9d9r"},"outputs":[],"source":["import re,io,os,sys\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Axx0cqGkraAL","executionInfo":{"status":"ok","timestamp":1667065518263,"user_tz":-480,"elapsed":17032,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"}}},"outputs":[],"source":["import re,io,os,sys\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","import torchtext,spacy\n","from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n","from torchtext.data.metrics import bleu_score\n","from torchtext.data.utils import get_tokenizer\n","from collections import Counter\n","from torchtext.vocab import vocab\n","import pandas as pd\n","import numpy as np\n","\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33307,"status":"ok","timestamp":1666880503970,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"l1od3qab8gDm","outputId":"d65131d6-4752-48ee-a82e-74ea27a33062"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30979,"status":"ok","timestamp":1666880534944,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"qxYpSjN6896L","outputId":"f9c7598b-0020-45c0-9995-e5bd2b3cf1b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting zh-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.4.0/zh_core_web_sm-3.4.0-py3-none-any.whl (48.4 MB)\n","\u001b[K     |████████████████████████████████| 48.4 MB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-sm==3.4.0) (3.4.2)\n","Collecting spacy-pkuseg<0.1.0,>=0.0.27\n","  Downloading spacy_pkuseg-0.0.32-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 15.0 MB/s \n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (8.1.5)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.0.9)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.4.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.7)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.10.2)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2022.9.24)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.1)\n","Installing collected packages: spacy-pkuseg, zh-core-web-sm\n","Successfully installed spacy-pkuseg-0.0.32 zh-core-web-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('zh_core_web_sm')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 15.3 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.1) (3.4.2)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!python -m spacy download zh_core_web_sm\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oq3eQspmfk26"},"outputs":[],"source":["\n","def getJsonFile(filePath,dir_path = \"/content/drive/MyDrive/Colab Notebooks/ithome/torchtext_anki/\"):\n","    return pd.read_json(dir_path+filePath, lines=True, orient='records')\n","\n","train_json = getJsonFile(\"anki_train.json\")\n","test_json = getJsonFile(\"anki_test.json\")\n","\n","total_json = pd.concat([train_json, test_json], axis=0)\n","\n","zh_tokenizer = get_tokenizer('spacy', language='zh_core_web_sm')\n","en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n","\n","def tokenize_eng(text):\n","    text = re.sub(r\"([.!?])\", r\" \\1\", text.lower())\n","    return en_tokenizer(text)\n","\n","def tokenize_zh(text):\n","    # print(text)\n","    regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n","    text = regex.sub(' ', text.lower())\n","    return zh_tokenizer(text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10615,"status":"ok","timestamp":1666880634296,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"NiOTGOpq9a01","outputId":"fd417cd8-bae8-48fd-f767-e12a4d3a40b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["中文語料的字元表長度:  14461 , 英文的字元表長度:  6933\n","<bos>\n","4291\n"]}],"source":["\n","def build_vocab(sentence_list, tokenizer):\n","  counter = Counter()\n","  for string_ in sentence_list:\n","    counter.update(tokenizer(string_))\n","  return vocab(counter, min_freq =1 , specials=['<unk>', '<bos>', '<eos>', '<pad>'])\n","\n","# chinese.build_vocab(train_data, max_size=50000, min_freq=50, vectors=\"glove.6B.100d\")\n","en_vocab = build_vocab(total_json.English, tokenize_eng)\n","zh_vocab = build_vocab(total_json.Chinese, tokenize_zh)\n","\n","\n","print (\"中文語料的字元表長度: \" , len(zh_vocab.vocab) , \", 英文的字元表長度: \" ,len(en_vocab.vocab))\n","\n","print(en_vocab.vocab.get_itos()[1])\n","print(en_vocab.vocab.get_stoi()['arabic'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12642,"status":"ok","timestamp":1666880646935,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"DsP6JVaC3dRV","outputId":"22d28c00-f9e4-48a5-b714-65e235d2bae4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample English: ['我', '最近', '忙', '得', '很'] => Chinese: ['i', \"'ve\", 'been', 'very', 'busy', 'lately', '.']\n"]}],"source":["\n","def data_process(sentence_list):\n","  data = []\n","  for _ , s in sentence_list.iterrows():\n","    data.append((tokenize_zh(s.Chinese),  tokenize_eng(s.English)))\n","    # data.append({\"zh\" : tokenize_zh(s.Chinese), \"en\" : tokenize_eng(s.English) })\n","  return data\n","\n","train_data = data_process(train_json)\n","test_data = data_process(test_json)\n","\n","\n","print (\"Sample English:\", test_data[0][0] , \"=> Chinese:\", test_data[0][1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fy7JnW0fpLt"},"outputs":[],"source":["\n","def translate_sentence(model, sentence_token, zh_vocab, en_vocab, device, max_length=25):\n","\n","    # 加入開始符號跟結束符號，代表一個句子\n","    sentence = ['<bos>'] + sentence_token + ['<eos>']\n","\n","    # 然後把文字轉自 vactor\n","    text_to_indices = [zh_vocab.vocab.get_stoi()[token] for token in sentence]\n","\n","    # 再把 vactor list 轉換成 Tensor\n","    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n","\n","    # 先宣告 outputs ，然後裡面放一個開符號\n","    outputs = [en_vocab.vocab.get_stoi()[\"<bos>\"]]\n","\n","    for i in range(max_length):\n","        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n","\n","        with torch.no_grad():\n","            output = model(sentence_tensor, trg_tensor)\n","\n","        # 機率最大的數值再把它放進output\n","        best_guess = output.argmax(2)[-1, :].item()\n","        outputs.append(best_guess)\n","\n","        # 如果是結束字元 eos 的話就中斷不然會一直預測下去\n","        if best_guess == en_vocab.vocab.get_stoi()[\"<eos>\"]:\n","            break\n","\n","\n","    # 再把 vactor 轉成文字， itos = integer to string\n","    translated_sentence = [en_vocab.vocab.get_itos()[idx] for idx in outputs]\n","    # remove start token\n","    return translated_sentence[1:]\n","\n","\n","\n","def bleu(data, model, tokenize_zh, zh_vocab, en_vocab, device):\n","    model.eval()\n","    targets_corpus = []\n","    outputs_corpus = []\n","\n","    for (src_token, trg_token) in data:\n","\n","        prediction = translate_sentence(model, src_token, zh_vocab, en_vocab, device)\n","        prediction = prediction[:-1]  # remove <eos> token\n","\n","        targets_corpus.append([trg_token])\n","        outputs_corpus.append(prediction)\n","\n","\n","    return bleu_score(outputs_corpus,targets_corpus)\n","\n","def save_checkpoint(state, filename=\"/content/drive/MyDrive/Colab Notebooks/ithome/checkpoints/seq2seq_transformer.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUR3a6iX9zLD"},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        embedding_size,\n","        src_vocab_size,\n","        trg_vocab_size,\n","        src_pad_idx,\n","        num_heads,\n","        num_encoder_layers,\n","        num_decoder_layers,\n","        forward_expansion,\n","        dropout,\n","        max_len,\n","        device,\n","    ):\n","        super(Transformer, self).__init__()\n","        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n","        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n","        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n","        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n","\n","        self.device = device\n","\n","        # 呼叫內建的 transformer\n","        self.transformer = nn.Transformer(\n","            embedding_size,\n","            num_heads,\n","            num_encoder_layers,\n","            num_decoder_layers,\n","            forward_expansion,\n","            dropout,\n","        )\n","        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.src_pad_idx = src_pad_idx\n","\n","    def make_src_mask(self, src):\n","        src_mask = src.transpose(0, 1) == self.src_pad_idx\n","\n","        # (N, src_len)\n","        return src_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        src_seq_length, N = src.shape\n","        trg_seq_length, N = trg.shape\n","\n","        src_positions = (\n","            torch.arange(0, src_seq_length)\n","            .unsqueeze(1)\n","            .expand(src_seq_length, N)\n","            .to(self.device)\n","        )\n","\n","        trg_positions = (\n","            torch.arange(0, trg_seq_length)\n","            .unsqueeze(1)\n","            .expand(trg_seq_length, N)\n","            .to(self.device)\n","        )\n","\n","        embed_src = self.dropout(\n","            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n","        )\n","        embed_trg = self.dropout(\n","            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n","        )\n","\n","        src_padding_mask = self.make_src_mask(src)\n","        # 訓練的時候要加入mask 不然會看到後面的答案\n","        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n","            self.device\n","        )\n","\n","        out = self.transformer(\n","            embed_src,\n","            embed_trg,\n","            src_key_padding_mask=src_padding_mask,\n","            tgt_mask=trg_mask,\n","        )\n","        out = self.fc_out(out)\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYNuumOtHkT1"},"outputs":[],"source":["checkPointPath = \"/content/drive/MyDrive/Colab Notebooks/ithome/checkpoints/seq2seq_transformer.pth.tar\"\n","\n","# We're ready to define everything we need for training our Seq2Seq model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# load_model = True\n","save_model = True\n","\n","# Training hyperparameters\n","num_epochs = 10000\n","learning_rate = 3e-4\n","batch_size = 32\n","\n","# Model hyperparameters\n","src_vocab_size = len(zh_vocab.vocab)\n","trg_vocab_size = len(en_vocab.vocab)\n","embedding_size = 512\n","num_heads = 8\n","num_encoder_layers = 3\n","num_decoder_layers = 3\n","dropout = 0.10\n","max_len = 100\n","forward_expansion = 4\n","src_pad_idx = en_vocab.vocab.get_stoi()[\"<pad>\"]\n","\n","# Tensorboard to get nice loss plot\n","writer = SummaryWriter(\"runs/loss_plot\")\n","step = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qchde2HMIKTf"},"outputs":[],"source":["\n","PAD_IDX = zh_vocab.vocab.get_stoi()['<pad>']\n","BOS_IDX = zh_vocab.vocab.get_stoi()['<bos>']\n","EOS_IDX = zh_vocab.vocab.get_stoi()['<eos>']\n","\n","\n","def generate_batch(data_batch):\n","  zh_batch, en_batch = [], []\n","  for (zh_token, en_token) in data_batch:\n","    zh_item = torch.tensor([zh_vocab.vocab.get_stoi()[token] for token in zh_token], dtype=torch.long)\n","    zh_batch.append(torch.cat([torch.tensor([BOS_IDX]), zh_item, torch.tensor([EOS_IDX])], dim=0))\n","\n","    en_item = torch.tensor([en_vocab.vocab.get_stoi()[token] for token in en_token], dtype=torch.long)\n","    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n","\n","  zh_batch = pad_sequence(zh_batch, padding_value=PAD_IDX)\n","  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n","  return zh_batch, en_batch\n","\n","train_iter = DataLoader(train_data, batch_size=batch_size,\n","                        shuffle=True, collate_fn=generate_batch)\n","\n","\n","test_iter = DataLoader(test_data, batch_size=batch_size,\n","                        shuffle=True, collate_fn=generate_batch)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30306,"status":"ok","timestamp":1666880677213,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"8hUEhKVoHrgH","outputId":"67baba2a-2062-433a-c3ba-1bdd72fb070a"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> Loading checkpoint\n","Bleu score 26.37\n"]}],"source":["\n","model = Transformer(\n","    embedding_size,\n","    src_vocab_size,\n","    trg_vocab_size,\n","    src_pad_idx,\n","    num_heads,\n","    num_encoder_layers,\n","    num_decoder_layers,\n","    forward_expansion,\n","    dropout,\n","    max_len,\n","    device,\n",").to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, factor=0.1, patience=10, verbose=True\n",")\n","\n","pad_idx = en_vocab.vocab.get_stoi()[\"<pad>\"]\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","\n","if os.path.isfile(checkPointPath):\n","    load_checkpoint(torch.load(checkPointPath), model, optimizer)\n","\n","model.eval()\n","# running on entire test data takes a while\n","\n","\n","score = bleu(test_data[1:100], model, tokenize_zh, zh_vocab, en_vocab, device)\n","print(f\"Bleu score {score*100:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"w8EFeBmRIFDS","outputId":"d0916acf-91a6-45c9-f977-c4c3e6b6192d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Epoch 0 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'go', 'to', 'my', 'family', '?', '<eos>']\n","[Epoch 1 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'have', 'a', 'supermarket', '?', '<eos>']\n","[Epoch 2 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'have', 'a', 'cat', '?', '<eos>']\n","[Epoch 3 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'go', 'to', 'my', 'house', '?', '<eos>']\n","[Epoch 4 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'come', 'to', 'my', 'house', '?', '<eos>']\n","[Epoch 5 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'you', 'want', 'to', 'come', 'to', 'my', 'house', 'on', 'a', 'cat', '?', '<eos>']\n","[Epoch 6 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'come', 'to', 'my', 'place', '?', '<eos>']\n","[Epoch 7 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'you', 'want', 'to', 'visit', 'my', 'house', 'on', 'twenty', '?', '<eos>']\n","[Epoch 8 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'come', 'to', 'my', 'house', '?', '<eos>']\n","[Epoch 9 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'come', 'to', 'my', 'cat', '?', '<eos>']\n","[Epoch 10 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'pick', 'me', 'up', 'the', 'house', '?', '<eos>']\n","[Epoch 11 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'see', 'your', 'parents', '?', '<eos>']\n","[Epoch 12 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'watch', 'the', 'cat', '?', '<eos>']\n","[Epoch 13 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'see', 'a', 'cat', '?', '<eos>']\n","[Epoch 14 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'come', 'over', 'after', 'my', 'house', '?', '<eos>']\n","[Epoch 15 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'come', 'from', 'my', 'house', '?', '<eos>']\n","[Epoch 16 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'see', 'my', 'cat', '?', '<eos>']\n","[Epoch 17 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'see', 'my', 'cat', '?', '<eos>']\n","[Epoch 18 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'come', 'and', 'my', 'cat', '?', '<eos>']\n","[Epoch 19 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['would', 'you', 'like', 'to', 'see', 'me', 'from', 'my', 'house', '?', '<eos>']\n","[Epoch 20 / 10000]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'you', 'want', 'to', 'see', 'me', 'every', 'night', '?', '<eos>']\n"]}],"source":["\n","sentence = \"你想要来我家看猫吗?\"\n","example_token = tokenize_zh(sentence)\n","\n","\n","for epoch in range(num_epochs):\n","    print(f\"[Epoch {epoch} / {num_epochs}]\")\n","\n","    if save_model:\n","        checkpoint = {\n","            \"state_dict\": model.state_dict(),\n","            \"optimizer\": optimizer.state_dict(),\n","        }\n","        save_checkpoint(checkpoint)\n","\n","    model.eval()\n","\n","    translated_sentence = translate_sentence(\n","        model, example_token, zh_vocab, en_vocab, device, max_length=50\n","    )\n","\n","    print(f\"Translated example sentence: \\n {translated_sentence}\")\n","    model.train()\n","    losses = []\n","\n","    for batch_idx, batch in enumerate(train_iter):\n","        # Get input and targets and get to cuda\n","        inp_data = batch[0].to(device)\n","        target = batch[1].to(device)\n","\n","        # Forward prop\n","        output = model(inp_data, target[:-1, :])\n","\n","        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n","        # doesn't take input in that form. For example if we have MNIST we want to have\n","        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n","        # way that we have output_words * batch_size that we want to send in into\n","        # our cost function, so we need to do some reshapin.\n","        # Let's also remove the start token while we're at it\n","        output = output.reshape(-1, output.shape[2])\n","        target = target[1:].reshape(-1)\n","\n","        optimizer.zero_grad()\n","\n","        loss = criterion(output, target)\n","        losses.append(loss.item())\n","\n","        # Back prop\n","        loss.backward()\n","        # Clip to avoid exploding gradient issues, makes sure grads are\n","        # within a healthy range\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","        # Gradient descent step\n","        optimizer.step()\n","\n","        # plot to tensorboard\n","        writer.add_scalar(\"Training loss\", loss, global_step=step)\n","        step += 1\n","\n","    mean_loss = sum(losses) / len(losses)\n","    scheduler.step(mean_loss)\n","\n","# running on entire test data takes a while\n","score = bleu(test_data[1:100], model, tokenize_zh, zh_vocab, en_vocab, device)\n","print(f\"Bleu score {score*100:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"rzottmYLKAw5"},"source":["# Next \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_KR_qhb516Y"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1zSu7vi5lcdb0xGGNFK0TyrJGo4LVEQsq","timestamp":1665903680308}],"mount_file_id":"1zSu7vi5lcdb0xGGNFK0TyrJGo4LVEQsq","authorship_tag":"ABX9TyMXprnnU9W8C7s7DiUXzhwA"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}