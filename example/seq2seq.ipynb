{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yg6pRS8B9T9-","outputId":"602b22b5-5497-4507-e519-591e68c9feb1","executionInfo":{"status":"ok","timestamp":1666140827828,"user_tz":-480,"elapsed":5182,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch==1.8.0+cu111 in /usr/local/lib/python3.7/dist-packages (1.8.0+cu111)\n","Requirement already satisfied: torchvision==0.9.0+cu111 in /usr/local/lib/python3.7/dist-packages (0.9.0+cu111)\n","Requirement already satisfied: torchaudio==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n","Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (4.1.1)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2022.9.24)\n"]}],"source":["# !pip install -U torch==1.8.0+cu111\n","!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 torchtext==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n","# Reload environment\n","exit()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1666141028862,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"ksS3EtrxfnIe"},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n","import numpy as np\n","import spacy\n","import random\n","from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n","# from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n","from torchtext.data.metrics import bleu_score\n","import sys\n","import os\n","import re\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666140943475,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"CnMXiA-PAKtO","outputId":"ad0487fd-8d3a-4d5f-94f5-70d9b07bb98f"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.8.0+cu111\n","0.9.0\n"]}],"source":["import torch, torchtext\n","print(torch.__version__)\n","print(torchtext.__version__)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14443,"status":"ok","timestamp":1666140957912,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"l1od3qab8gDm","outputId":"09929cc5-1842-4f67-f1f7-d950f71e0f1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17655,"status":"ok","timestamp":1666140975563,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"qxYpSjN6896L","outputId":"3a8d1c28-0461-4158-a9ea-8daf80cc6e48"},"outputs":[{"output_type":"stream","name":"stdout","text":["2022-10-19 00:56:03.453009: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting zh-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.4.0/zh_core_web_sm-3.4.0-py3-none-any.whl (48.4 MB)\n","\u001b[K     |████████████████████████████████| 48.4 MB 76 kB/s \n","\u001b[?25hCollecting spacy-pkuseg<0.1.0,>=0.0.27\n","  Downloading spacy_pkuseg-0.0.32-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from zh-core-web-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.0.9)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.8)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.7)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (8.1.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2022.9.24)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.1)\n","Installing collected packages: spacy-pkuseg, zh-core-web-sm\n","Successfully installed spacy-pkuseg-0.0.32 zh-core-web-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('zh_core_web_sm')\n"]}],"source":["!python -m spacy download zh_core_web_sm"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"oq3eQspmfk26","executionInfo":{"status":"ok","timestamp":1666141051118,"user_tz":-480,"elapsed":20656,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"36510914-042c-4100-8701-61ff752ba4f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["中文語料的字元表長度:  13535 , 英文的字元表長度:  6607\n","Sample English: ['i', \"'ve\", 'been', 'very', 'busy', 'lately', '.'] => Chinese: ['我', '最近', '忙', '得', '很']\n"]}],"source":["spacy_zh = spacy.load(\"zh_core_web_sm\")\n","spacy_eng = spacy.load(\"en_core_web_sm\")\n","\n","def tokenize_eng(text):\n","    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n","    return [tok.text for tok in spacy_eng.tokenizer(text)]\n","\n","\n","def tokenize_zh(text):\n","    regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n","    text = regex.sub(' ', text)\n","\n","    return [tok.text for tok in spacy_zh.tokenizer(text)]\n","\n","\n","\n","english = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, lower=True)\n","chinese = Field(sequential=True, use_vocab=True, tokenize=tokenize_zh, lower=True)\n","\n","fields = {\"English\": (\"eng\", english), \"Chinese\": (\"zh\", chinese)}\n","\n","train_data, val_data, test_data = TabularDataset.splits(\n","    path=\"/content/drive/MyDrive/Colab Notebooks/ithome/torchtext_anki/\", \n","    train=\"anki_train.json\", \n","    validation=\"anki_val.json\", \n","    test=\"anki_test.json\", \n","    format=\"json\", \n","    fields=fields\n",")\n","\n","\n","english.build_vocab(train_data,min_freq=1)\n","chinese.build_vocab(train_data,min_freq=1)\n","\n","print (\"中文語料的字元表長度: \" , len(chinese.vocab) , \", 英文的字元表長度: \" ,len(english.vocab))\n","print (\"Sample English:\", test_data[0].eng , \"=> Chinese:\", test_data[0].zh)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3507,"status":"ok","timestamp":1665857169555,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"NiOTGOpq9a01","outputId":"1554ce74-cba0-4aac-91f2-df3be82fa3b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["10002\n","10002\n"]}],"source":["\n","# english.build_vocab(train_data, max_size=50000, min_freq=20, vectors=\"glove.6B.100d\")\n","# chinese.build_vocab(train_data, max_size=50000, min_freq=50, vectors=\"glove.6B.100d\")\n","\n","chinese.build_vocab(train_data, max_size=10000, min_freq=2)\n","english.build_vocab(train_data, max_size=10000, min_freq=2)\n","print(len(english.vocab))\n","print(len(chinese.vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fy7JnW0fpLt"},"outputs":[],"source":["\n","\n","def translate_sentence(model, sentence, chinese, english, device, max_length=25):\n","    \n","    # 先載入 tokensizer\n","    spacy_zh = spacy.load(\"zh_core_web_sm\")\n","\n","    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n","    if type(sentence) == str:\n","        tokens = [token.text.lower() for token in spacy_zh(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","\n","\n","    # 加入開始符號跟結束符號，代表一個句子\n","    tokens.insert(0, chinese.init_token)\n","    tokens.append(chinese.eos_token)\n","\n","    # 然後把文字轉自 vactor\n","    text_to_indices = [chinese.vocab.stoi[token] for token in tokens]\n","\n","    # 再把 vactor list 轉換成 Tensor\n","    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n","\n","    # 取消梯度修正\n","    with torch.no_grad():\n","        hidden, cell = model.encoder(sentence_tensor)\n","\n","    # 先宣告 outputs ，然後裡面放一個開符號\n","    outputs = [english.vocab.stoi[\"<sos>\"]]\n","\n","\n","    for _ in range(max_length):\n","        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n","\n","        # seq2seq 的decoder 會把上一個 hidden_state 跟 cell 當作input 這是觀念\n","        # 然後output機率最大的\n","        with torch.no_grad():\n","            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n","            best_guess = output.argmax(1).item()\n","\n","        # 機率最大的數值再把它放進output\n","        outputs.append(best_guess)\n","\n","        # 如果是結束字元 eos 的話就中斷不然會一直預測下去\n","        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n","            break\n","\n","    # 再把 vactor 轉成文字， itos = integer to string\n","    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n","\n","    # remove start token\n","    return translated_sentence[1:]\n","\n","\n","\n","def bleu(data, model, chinese, english, device):\n","    targets = []\n","    outputs = []\n","\n","    for example in data:\n","        src = vars(example)[\"zh\"]\n","        trg = vars(example)[\"eng\"]\n","\n","        prediction = translate_sentence(model, src, chinese, english, device)\n","        prediction = prediction[:-1]  # remove <eos> token\n","\n","        targets.append([trg])\n","        outputs.append(prediction)\n","\n","    return bleu_score(outputs, targets)\n","\n","\n","def save_checkpoint(state, filename=\"/content/drive/MyDrive/Colab Notebooks/ithome/seq2seq_cp.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUR3a6iX9zLD"},"outputs":[],"source":["\n","class Encoder(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n","        super(Encoder, self).__init__()\n","        self.dropout = nn.Dropout(p)\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n","\n","    def forward(self, x):\n","        # x shape: (seq_length, N) where N is batch size\n","\n","        embedding = self.dropout(self.embedding(x))\n","        # embedding shape: (seq_length, N, embedding_size)\n","\n","        outputs, (hidden, cell) = self.rnn(embedding)\n","        # outputs shape: (seq_length, N, hidden_size)\n","\n","        return hidden, cell\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFwTb4UP901T"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(\n","        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n","    ):\n","        super(Decoder, self).__init__()\n","        self.dropout = nn.Dropout(p)\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, hidden, cell):\n","        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n","        # is 1 here because we are sending in a single word and not a sentence\n","        x = x.unsqueeze(0)\n","\n","        embedding = self.dropout(self.embedding(x))\n","        # embedding shape: (1, N, embedding_size)\n","\n","        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n","        # outputs shape: (1, N, hidden_size)\n","\n","        predictions = self.fc(outputs)\n","\n","        # predictions shape: (1, N, length_target_vocabulary) to send it to\n","        # loss function we want it to be (N, length_target_vocabulary) so we're\n","        # just gonna remove the first dim\n","        predictions = predictions.squeeze(0)\n","\n","        return predictions, hidden, cell\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VEYNddf93ww"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, source, target, teacher_force_ratio=0.5):\n","        batch_size = source.shape[1]\n","        target_len = target.shape[0]\n","        target_vocab_size = len(english.vocab)\n","\n","        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","\n","        hidden, cell = self.encoder(source)\n","\n","        # Grab the first input to the Decoder which will be <SOS> token\n","        x = target[0]\n","\n","        for t in range(1, target_len):\n","            # Use previous hidden, cell as context from encoder at start\n","            output, hidden, cell = self.decoder(x, hidden, cell)\n","\n","            # Store next output prediction\n","            outputs[t] = output\n","\n","            # Get the best word the Decoder predicted (index in the vocabulary)\n","            best_guess = output.argmax(1)\n","\n","            # With probability of teacher_force_ratio we take the actual next word\n","            # otherwise we take the word that the Decoder predicted it to be.\n","            # Teacher Forcing is used so that the model gets used to seeing\n","            # similar inputs at training and testing time, if teacher forcing is 1\n","            # then inputs at test time might be completely different than what the\n","            # network is used to. This was a long comment.\n","            x = target[t] if random.random() < teacher_force_ratio else best_guess\n","\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2C4ax1l-DZ1"},"outputs":[],"source":["checkPointPath = \"/content/drive/MyDrive/Colab Notebooks/ithome/seq2seq_cp.pth.tar\"\n","\n","\n","num_epochs = 100\n","learning_rate = 0.001\n","batch_size = 64\n","\n","# Model hyperparameters\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","input_size_encoder = len(chinese.vocab)\n","input_size_decoder = len(english.vocab)\n","output_size = len(english.vocab)\n","encoder_embedding_size = 300\n","decoder_embedding_size = 300\n","hidden_size = 1024 # Needs to be the same for both RNN's\n","num_layers = 2\n","enc_dropout = 0.5\n","dec_dropout = 0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8fmFdm--FpU"},"outputs":[],"source":["\n","# Tensorboard to get nice loss plot\n","writer = SummaryWriter(f\"/content/drive/MyDrive/Colab Notebooks/ithome/runs/loss_plot\")\n","step = 0\n","\n","train_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, test_data),\n","    batch_size=batch_size,\n","    sort_within_batch=True,\n","    sort_key = lambda x: len(x.zh),\n","    device=device,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKCsMQV8-QII"},"outputs":[],"source":["\n","encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\n","\n","decoder_net = Decoder(\n","    input_size_decoder,\n","    decoder_embedding_size,\n","    hidden_size,\n","    output_size,\n","    num_layers,\n","    dec_dropout,\n",").to(device)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3298,"status":"ok","timestamp":1665857178568,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"},"user_tz":-480},"id":"C5neDfUY-cfQ","outputId":"0997738a-bcd4-4b0c-ff1a-55333c8b4ec5"},"outputs":[{"output_type":"stream","name":"stdout","text":["=> Loading checkpoint\n"]}],"source":["\n","model = Seq2Seq(encoder_net, decoder_net).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","pad_idx = english.vocab.stoi[\"<pad>\"]\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","\n","if os.path.isfile(checkPointPath):\n","    load_checkpoint(torch.load(checkPointPath), model, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dyaFkzqH9JvE","outputId":"aae1db58-85a0-4500-e338-bd1fe8df7f02","executionInfo":{"status":"ok","timestamp":1665903010895,"user_tz":-480,"elapsed":45711514,"user":{"displayName":"蔡岳峻","userId":"15002473577582326109"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 0 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'try', 'to', 'want', 'you', 'to', 'make', 'a', '<unk>']\n","[Epoch 1 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'block', 'or', 'does', 'not', 'want', 'to', 'make', 'a', '<unk>']\n","[Epoch 2 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'reluctance', 'to', 'work', 'you', 'want', 'to', 'make', 'a', 'wear', 'or', '<unk>']\n","[Epoch 3 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['has', 'not', 'want', 'to', 'to', 'to', 'to', 'a', 'a', '<unk>']\n","[Epoch 4 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['or', 'not', 'want', 'to', '<unk>']\n","[Epoch 5 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'want', 'to', 'want', 'to', 'make', 'a', '<unk>']\n","[Epoch 6 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['or', 'not', 'want', 'to', 'do', 'something', 'to', 'a', 'a', 'ride', 'on', 'a', '<unk>']\n","[Epoch 7 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['or', 'not', 'want', 'to', 'do', 'what', 'you', 'want', 'to', 'do', 'or', 'think', 'things', 'like', 'to', 'or', 'to', '<unk>']\n","[Epoch 8 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'make', 'you', 'want', 'to', 'build', 'a', 'or', 'or', '<unk>']\n","[Epoch 9 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'make', 'or', '<unk>']\n","[Epoch 10 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'make', 'big', 'a', 'to', 'to', 'wear', 'a', 'or', 'a', 'or', 'a', '<unk>']\n","[Epoch 11 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['’s', '<unk>']\n","[Epoch 12 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'want', 'to', 'make', 'a', '<unk>']\n","[Epoch 13 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['’s', '<unk>']\n","[Epoch 14 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['’s', 'thinking', 'that', 'you', 'commit', 'to', 'a', 'or', 'or', 'or', 'or', 'you', 'or', '?', '?', '?', '?', '?', 'or', '?', '?', '?', '?', '?', '?', '.', '.', 'register', 'a', 'migrant', 'institutes', '.', 'the', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n","[Epoch 15 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['’s', '<unk>']\n","[Epoch 16 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['is', 'little', 'hope', 'that', 'you', 'can', 'not', 'build', 'a', 'or', 'or', 'a', '<unk>']\n","[Epoch 17 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['has', 'not', 'want', 'to', 'make', 'sure', 'that', 'giving', 'up', 'to', 'a', 'or', 'a', 'to', 'a', 'a', '<unk>']\n","[Epoch 18 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'want', 'to', 'see', 'a', '<unk>']\n","[Epoch 19 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'want', 'to', 'see', 'a', '<unk>']\n","[Epoch 20 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'reluctance', 'to', 'fire', 'the', '<unk>']\n","[Epoch 21 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'see', 'a', '<unk>']\n","[Epoch 22 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'see', 'a', '<unk>']\n","[Epoch 23 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'go', 'a', 'vacation', 'home', 'to', 'you', 'want', 'to', 'to', 'a', 'a', 'or', 'a', 'locals', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'register', 'a', 'fire', '.', 'the', 'massachusetts', '.', 'the', 'parent', 'of', 'which', 'the', 'tree', 'is', 'massachusetts', '.', '”', '<unk>']\n","[Epoch 24 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['this', 'idea', 'does', 'not', 'want', 'to', 'see', 'a', '<unk>']\n","[Epoch 25 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'yearning', 'want', 'to', '<unk>']\n","[Epoch 26 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'end', 'up', 'a', 'a', 'a', 'a', 'or', 'a', 'a', 'a', 'a', '<unk>']\n","[Epoch 27 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', '<unk>']\n","[Epoch 28 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'make', 'a', 'wire', 'or', 'a', 'a', '<unk>']\n","[Epoch 29 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'stressed', 'her', 'to', 'transcend', 'or', '<unk>']\n","[Epoch 30 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'daughters', 'to', 'want', 'to', 'make', 'a', '<unk>']\n","[Epoch 31 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'want', 'to', 'sake', 'of', 'eating', 'or', 'or', 'you', 'want', 'to', 'to', 'to', 'or', '<unk>']\n","[Epoch 32 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'daughters', 'want', 'to', 'make', 'a', 'a', 'small', '<unk>']\n","[Epoch 33 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'want', 'to', 'do', 'a', '<unk>']\n","[Epoch 34 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'see', 'a', '<unk>']\n","[Epoch 35 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['wants', 'any', 'desire', 'to', 'want', 'a', 'to', 'settle', 'for', 'a', 'or', 'or', 'a', 'locals', 'or', 'a', '<unk>']\n","[Epoch 36 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['can', 'try', 'to', 'tap', 'the', 'extra', '<unk>']\n","[Epoch 37 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['has', 'hopes', 'that', 'you', 'can', 'not', 'wish', 'to', 'to', 'to', 'or', 'or', 'or', 'or', 'or', '<unk>']\n","[Epoch 38 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'unwilling', 'to', '<unk>']\n","[Epoch 39 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctance', 'to', 'give', 'up', 'a', 'a', 'to', 'a', 'or', 'or', 'a', '<unk>']\n","[Epoch 40 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctance', 'to', 'help', 'build', 'small', '<unk>']\n","[Epoch 41 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'pledge', 'that', 'you', 'want', 'want', 'to', 'send', 'a', 'to', 'the', 'or', 'or', 'or', 'or', 'to', '<unk>']\n","[Epoch 42 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'do', 'want', 'to', 'go', 'to', '<unk>']\n","[Epoch 43 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'daughters', 'do', 'not', 'want', 'to', 'see', 'a', '<unk>']\n","[Epoch 44 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'daughters', 'do', 'not', 'want', 'to', 'see', 'the', '<unk>']\n","[Epoch 45 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['has', 'no', 'hope', 'that', 'you', 'can', 'not', 'commit', 'to', 'to', 'a', 'a', '<unk>']\n","[Epoch 46 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['was', 'you', 'to', 'want', 'to', 'see', 'the', '<unk>']\n","[Epoch 47 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['can', 'not', 'want', 'to', 'do', '<unk>']\n","[Epoch 48 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'want', 'to', 'wear', 'a', '<unk>']\n","[Epoch 49 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'reluctance', 'to', 'help', 'tap', 'the', 'ride', 'on', 'your', '<unk>']\n","[Epoch 50 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'want', 'to', 'commit', 'to', '<unk>']\n","[Epoch 51 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['was', 'right', 'willing', 'to', 'do', 'what', 'you', 'wish', 'to', 'to', 'to', 'or', 'or', 'a', '<unk>']\n","[Epoch 52 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'do', 'you', 'want', 'to', 'wear', 'a', '<unk>']\n","[Epoch 53 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'do', 'something', 'to', 'a', 'a', 'master', 'or', 'a', '<unk>']\n","[Epoch 54 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'do', '<unk>']\n","[Epoch 55 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'do', 'a', '<unk>']\n","[Epoch 56 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['can', 'try', 'to', 'tap', 'the', '<unk>']\n","[Epoch 57 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'reluctance', 'to', 'commit', 'you', 'to', 'wear', 'a', '<unk>']\n","[Epoch 58 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'do', 'something', 'that', 'a', 'to', 'wear', 'a', '<unk>']\n","[Epoch 59 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'do', 'to', 'to', 'wear', 'a', '<unk>']\n","[Epoch 60 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', '<unk>']\n","[Epoch 61 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'do', 'want', 'to', 'see', 'the', '<unk>']\n","[Epoch 62 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'to', 'want', 'a', '<unk>']\n","[Epoch 63 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', '<unk>']\n","[Epoch 64 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'do', 'what', 'you', 'want', 'to', 'or', 'or', 'a', '<unk>']\n","[Epoch 65 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'a', '<unk>']\n","[Epoch 66 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'to', 'do', 'wire', 'to', 'do', 'a', '<unk>']\n","[Epoch 67 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'you', 'want', 'to', 'do', 'to', 'to', 'wear', 'a', '<unk>']\n","[Epoch 68 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'n’t', 'want', 'to', 'do', 'what', 'you', 'want', 'to', 'see', 'in', 'a', '<unk>']\n","[Epoch 69 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'you', 'want', 'to', '<unk>']\n","[Epoch 70 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctant', 'to', '<unk>']\n","[Epoch 71 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctant', 'to', 'ask', 'for', 'a', '<unk>']\n","[Epoch 72 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctant', 'to', '<unk>']\n","[Epoch 73 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'not', 'want', 'you', 'want', 'to', 'to', 'a', 'ride', 'on', 'a', 'a', '<unk>']\n","[Epoch 74 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctant', 'to', '<unk>']\n","[Epoch 75 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctant', 'to', '<unk>']\n","[Epoch 76 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'want', 'to', 'try', 'to', 'bus', 'or', '<unk>']\n","[Epoch 77 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctant', 'to', 'want', 'to', '<unk>']\n","[Epoch 78 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctant', 'to', '<unk>']\n","[Epoch 79 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'pledge', 'to', 'want', 'to', 'make', 'or', 'eat', 'to', 'or', 'or', 'to', 'to', 'to', 'or', 'to', 'variations', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n","[Epoch 80 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'hoping', 'to', 'kill', 'a', '<unk>']\n","[Epoch 81 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctant', 'to', '<unk>']\n","[Epoch 82 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'to', 'survive', 'the', '<unk>']\n","[Epoch 83 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'n’t', 'want', 'to', 'see', 'to', '<unk>']\n","[Epoch 84 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'to', 'play', 'a', '<unk>']\n","[Epoch 85 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctance', 'to', 'give', 'up', 'wanting', 'a', 'a', 'to', 'tightly', 'to', 'to', 'a', '<unk>']\n","[Epoch 86 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'to', 'survive', 'the', '<unk>']\n","[Epoch 87 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'want', 'to', 'see', 'wire', 'drastically', '<unk>']\n","[Epoch 88 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'want', 'to', 'see', 'a', '<unk>']\n","[Epoch 89 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctance', 'to', 'help', 'you', 'wanted', 'a', '<unk>']\n","[Epoch 90 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'want', 'to', 'work', 'a', 'ride', 'on', 'a', '<unk>']\n","[Epoch 91 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'wanted', 'to', 'do', 'to', 'to', 'a', 'a', '<unk>']\n","[Epoch 92 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'to', 'commit', 'a', '<unk>']\n","[Epoch 93 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'want', 'to', 'get', 'a', '<unk>']\n","[Epoch 94 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'reluctant', 'to', 'play', 'the', '<unk>']\n","[Epoch 95 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'n’t', 'want', 'to', 'get', 'the', '<unk>']\n","[Epoch 96 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'you', 'commit', 'to', '<unk>']\n","[Epoch 97 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['did', 'you', 'want', 'hoping', 'that', 'a', '<unk>']\n","[Epoch 98 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'not', 'want', 'to', 'do', 'the', 'boat', '<unk>']\n","[Epoch 99 / 100]\n","=> Saving checkpoint\n","Translated example sentence: \n"," ['do', 'wanted', 'to', 'do', 'without', 'a', 'without', 'a', '<unk>']\n","Bleu score 2.49\n"]}],"source":["\n","sentence = \"你想不想要来我家看猫?或者一起看Netflix?\"\n","\n","for epoch in range(num_epochs):\n","    print(f\"[Epoch {epoch} / {num_epochs}]\")\n","\n","    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n","    save_checkpoint(checkpoint)\n","\n","    model.eval()\n","\n","    translated_sentence = translate_sentence(\n","        model, sentence, chinese, english, device, max_length=50\n","    )\n","\n","    print(f\"Translated example sentence: \\n {translated_sentence}\")\n","\n","    model.train()\n","\n","    for batch_idx, batch in enumerate(train_iterator):\n","        # Get input and targets and get to cuda\n","        inp_data = batch.zh.to(device)\n","        target = batch.eng.to(device)\n","\n","        # Forward prop\n","        output = model(inp_data, target)\n","        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n","        # doesn't take input in that form. For example if we have MNIST we want to have\n","        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n","        # way that we have output_words * batch_size that we want to send in into\n","        # our cost function, so we need to do some reshapin. While we're at it\n","        # Let's also remove the start token while we're at it\n","        output = output[1:].reshape(-1, output.shape[2])\n","        target = target[1:].reshape(-1)\n","\n","        optimizer.zero_grad()\n","        loss = criterion(output, target)\n","\n","        # Back prop\n","        loss.backward()\n","\n","        # Clip to avoid exploding gradient issues, makes sure grads are\n","        # within a healthy range\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","        # Gradient descent step\n","        optimizer.step()\n","\n","        # Plot to tensorboard\n","        writer.add_scalar(\"Training loss\", loss, global_step=step)\n","        step += 1\n","\n","\n","score = bleu(test_data[1:100], model, chinese, english, device)\n","print(f\"Bleu score {score*100:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYKI3PhnAWay"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"mount_file_id":"1zSu7vi5lcdb0xGGNFK0TyrJGo4LVEQsq","authorship_tag":"ABX9TyNHKqd3OGwwVV05k1VS2aPf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}